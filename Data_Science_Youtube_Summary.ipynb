{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pXW-UtwU_yUY",
        "zxhG2G1kMXGo",
        "kmCXQ6kVUR5f",
        "4ND5KEwoUbcf",
        "lX39XWA5l5wR",
        "YFJJot5al9qE",
        "IO-ppQoKELxG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1 style=\"text-align:center;\"><font color='red' size=10><b> Before you start! </b></font></h1>\n",
        "\n",
        "This project is divided in 3 parts:\n",
        "\n",
        "1) ETL, where we will Extract, Transform and Load data into an AWS RDS.\n",
        "\n",
        "https://colab.research.google.com/drive/1w9bPc49joLrceMWAF_RlEgXo73s1Eeee?usp=sharing\n",
        "\n",
        "<br><br>\n",
        "\n",
        "2) Data Analysis: exploratory data analysis to identify key features.\n",
        "\n",
        "https://colab.research.google.com/drive/1a_Etj5kwEaq5epwoV9TVPNS3ShRRd7wU?usp=sharing\n",
        "\n",
        "<br><br>\n",
        "\n",
        "3) Prediction models: model building and comparison.\n",
        "\n",
        "https://colab.research.google.com/drive/1Nbj6TM5HaK2krMRa9R1oiolDhDronYQB?usp=sharing\n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "**Summary of this project: https://colab.research.google.com/drive/1CUjP7SdFGldPjuSVSIbHAk9UPWYp_RYz?usp=sharing  <font color='red'>-> you are here</font>**\n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "\n",
        "A summary of the data can be visualized on this <font color='red'>**Power BI dashboard:**</font> https://app.powerbi.com/view?r=eyJrIjoiNTkzZjNmY2UtNmQ5Mi00MTJhLTliNzgtZGU2NzRlYzQ5NDA1IiwidCI6IjE2OGQ0MTM3LWQ2ZjYtNDVmOC1hYWE3LWQxYTcwMjMzMDk1ZSIsImMiOjR9&pageName=ReportSection4f69a4c8629ea033a165"
      ],
      "metadata": {
        "id": "Jn6p9_N8F8iQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analyzing Youtube channels that I am subscribed to using its API.**\n",
        "\n",
        "I love watching Youtube. It's a very diverse streaming service (even before this word was cool), with videos ranging from comedy to science and curiosity, short or long, from channels with millions of subscribers to small comunnities with just a few hundred. \n",
        "\n",
        "But content creators suffer from \"punitive algorithms\" (in their words) that increase or decrease their visibility based on a plethora of metrics, more often than ever unknown to them. From YT perspective it's actually understandable, since a lot of viewers are migrating to other platforms (Tiktok, Twitch and an ever growing streaming services options). They need to maximize the amount of viewers and time spent on each video to justify that Youtube is actually a good platform to place ads.\n",
        "\n",
        "With this in mind I decided to use its API to collect some data and try to predict a video \"view count\" at the time it was published. Some models were built using video title features, thumbnail color, day and hour it was published, and so on. If successful, it could be used for example to fine tune ad placement in terms of expected return it will give. If unsuccessful it can at least be improved in future iterations."
      ],
      "metadata": {
        "id": "yQEFZpNHZhLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1) ETL**"
      ],
      "metadata": {
        "id": "dHQHqMP-rK8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*You'll need a Youtube API key and a Postgres DB on AWS RDS. The API key, db login and password needs to be inside a .env file in the first notebook.*\n",
        "\n",
        "In this part I used Youtube's API to request and extract data from a list of 10 channels and videos that those channels published in a given period. Around 2.500 videos were extracted and store in a pandas dataframe.\n",
        "\n",
        "Then I used regular expressions to extract the amount of words, \"?\", \"!\" and \"...\" symbols of each title. The next step was to transform video duration from ISO 8601 to seconds and then extracted the main color of each thumbnail. All of this was also store in the same df.\n",
        "\n",
        "Finally, I used psycopg2 to connect to a Postgres DB on AWS RDS. Videos/channels are updated (if they already exist in the DB) or inserted (if they are new) in the DB. I simply divided the DB in two tables: \"videos\" and \"channels\"."
      ],
      "metadata": {
        "id": "Enqqxgy4qzQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2) Data analysis**"
      ],
      "metadata": {
        "id": "wmSBPe6Vt8_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook I use a read-only user from the db to read data. Two transformations are needed: extract the name of the day from upload date, and the **local** upload hour (it should consider the timezone it was published on!!)\n",
        "\n",
        "*  2857 videos are in the database (it's impressive, but some have the exact same title!).\n",
        "*  View count, like count and comment count are all positively skewed.\n",
        "*  Videos can be divided in two groups: one with many videos with a small amount of views, likes and comments; another one with less videos but with way more views, likes and comments. This can be more easily seen using the log distribution.\n",
        "*  Videos are mostly published on Tuesday/Thursday, around lunch time and early evening.\n",
        "*  Channel \"UMotivo\" has almost half of all videos in the database, and it could create biases. This reinforces the need to analyze each channel individually (it doesn't need to be in detail though).\n",
        "  *  Video count (the amout of videos a channel has published) is positively correlated with the number of exclamation marks on titles. Since video count is a feature that is constant for each channel, this can be analyzed as -> channels that publishes more tend to use more exclamation marks on video titles. That's precisely the case for the channel \"UMotivo\" for example.\n",
        "*  There's no clear pattern on day of the week or hour of the day a video was published and the amount of views it's going to have.\n",
        "*  As expected, view count is positively correlated with like and comment count.\n",
        "*  Subscriber count is positively correlated with like count; more subscribers equal more people who naturally like to watch a YT creator, so they naturally use the \"like\" button more.\n",
        "\n",
        "When analyzing channels individually:\n",
        "*  Some channels have a view count distribution curve that is a normal distribution, while others follow a power law.\n",
        "*  Channels are really particular when choosing the day of the week they are going to publish, but all channels publishes around lunch time and early evening.\n",
        "\n",
        "It was really hard to draw conclusions of which features the machine learning models would benefit more from. But all correlation plots indicates that the main color of a thumbnail has little importance. Nonetheless, models will be built using different features."
      ],
      "metadata": {
        "id": "YrkFmhXdt99P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3) Prediction models**"
      ],
      "metadata": {
        "id": "_UHbzY_7u9fC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Several prediction models were created, using different machine learning techniques:\n",
        "*   A dummy model (simply the median of view count by channel)\n",
        "*   Baseline model: Decision Tree Regressor\n",
        "*   Challenger model: Histogram Gradient Boosting Regressor\n",
        "*   Challenger model: Random Forest\n",
        "\n",
        "\n",
        "When analyzing the data it helped a lot to use the log of view counts, so the model will be built exactly the same, that is, it will try to predict the amount of log views. The code is simply enough to use a function 'f' to transform the output and the corresponding 'f inverse' to transform it back to the original format.\n",
        "\n",
        "I have created a prediciton pretty print method to convert back to the original format if it needs to.\n",
        "\n",
        "The final result of each method is:"
      ],
      "metadata": {
        "id": "0qBv-9-tvATN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **To-do**\n",
        "\n",
        "I would like to work on other parts of this project:\n",
        "\n",
        "\n",
        "1.   Get the number of words in the thumbnail (as well as features of these words)\n",
        "2.   Use CV to count the number of human faces in the thumbnail (and possibly its emotion)\n",
        "3.   Check if there are flashy elements in the thumbnail (red arrows for example)\n",
        "4.   Redo models using the new features\n",
        "5.   Create individual models for **each** channel (this would be good to account for unbalanced dataset in terms of channels with a lot of videos published and channels with only a few). When trying to predict view count of a video from a channel not in the db, use the most similar channel model (or a combination of all models). Still need to think about this.\n",
        "\n"
      ],
      "metadata": {
        "id": "J5pupetUSZju"
      }
    }
  ]
}